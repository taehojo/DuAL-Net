#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import argparse
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
from matplotlib.backends.backend_pdf import PdfPages

try:
    from src.utils import Logger, remove_zv_by_idx, get_zero_var_idx
    from src.annotation_transformer import transform_annotation_advanced
    from src.analysis import local_analysis, calculate_global_annotation, alpha_search
    from src.plotting import plot_roc_for_n
except ImportError:
    PACKAGE_PARENT = '..'
    SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser(__file__))))
    sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))
    try:
        from src.utils import Logger, remove_zv_by_idx, get_zero_var_idx
        from src.annotation_transformer import transform_annotation_advanced
        from src.analysis import local_analysis, calculate_global_annotation, alpha_search
        from src.plotting import plot_roc_for_n
    except ImportError as e:
         print(f"Error importing modules. Make sure the script is run from the project root or adjust Python path.")
         print(e)
         sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="Run DuAL-Net Local and Global Analysis Pipeline.")
    parser.add_argument("raw_data_file", help="Path to the raw SNP data file (e.g., plink .raw format).")
    parser.add_argument("dx_data_file", help="Path to the diagnosis/phenotype file.")
    parser.add_argument("annotation_file", help="Path to the SNP annotation CSV file (generated by annotate_snps.py).")
    parser.add_argument("--window_size", type=int, default=100, help="Window size for local analysis (default: 100).")
    parser.add_argument("--prefix", default="dual_net_output", help="Prefix for output files (default: dual_net_output).")
    parser.add_argument("--tabnet_epoch", type=int, default=50, help="Max epochs for TabNet training (default: 50).")
    parser.add_argument("--tabnet_patience", type=int, default=10, help="Patience for TabNet early stopping (default: 10).")
    parser.add_argument("--n_splits", type=int, default=5, help="Number of folds for cross-validation/OOF (default: 5).")
    parser.add_argument("--result_dir", default="result", help="Directory to save results (default: result).")
    parser.add_argument("--roc_n_values", nargs='+', type=int, default=[50, 100, 500, 1000, 2000],
                        help="List of N values for Top/Bottom/Random ROC plots (default: 50 100 500 1000 2000).")
    parser.add_argument("--alpha_top_n", type=int, default=100, help="Number of top SNPs to use for evaluating alpha (default: 100).")
    parser.add_argument("--alpha_step", type=float, default=0.1, help="Step size for alpha search (default: 0.1).")

    args = parser.parse_args()

    os.makedirs(args.result_dir, exist_ok=True)
    log_file_path = os.path.join(args.result_dir, f"{args.prefix}_analysis_logs.txt")
    logger = Logger(log_file_path)
    logger.log("Starting DuAL-Net Analysis Pipeline...")
    logger.log(f"Arguments: {vars(args)}")

    logger.log(f"Loading raw SNP data from: {args.raw_data_file}")
    try:
        raw_df = pd.read_csv(args.raw_data_file, sep=r'\s+')
        drop_cols_raw = ['FID', 'IID', 'PAT', 'MAT', 'SEX', 'PHENOTYPE']
        cols_to_drop_raw = [c for c in drop_cols_raw if c in raw_df.columns]
        if cols_to_drop_raw:
            raw_df.drop(columns=cols_to_drop_raw, inplace=True)
            logger.log(f"Dropped standard columns: {cols_to_drop_raw}")
        raw_df.columns = [c.split('_')[0] for c in raw_df.columns]
        logger.log(f"Loaded raw data: {raw_df.shape[0]} samples, {raw_df.shape[1]} SNPs")
        zv_raw_cols = get_zero_var_idx(raw_df)
        if zv_raw_cols:
             logger.log(f"Warning: Found {len(zv_raw_cols)} zero-variance SNPs in the initial raw data. These might be removed during analysis steps.")
    except FileNotFoundError:
        logger.log(f"Error: Raw data file not found at {args.raw_data_file}")
        logger.write()
        sys.exit(1)
    except Exception as e:
        logger.log(f"Error loading or processing raw data file: {e}")
        logger.write()
        sys.exit(1)

    logger.log(f"Loading diagnosis data from: {args.dx_data_file}")
    try:
        dx_df = pd.read_csv(args.dx_data_file, sep=r'\s+')
        target_col = 'New_Label'
        if target_col not in dx_df.columns:
             if 'PHENOTYPE' in dx_df.columns:
                 target_col = 'PHENOTYPE'
                 logger.log(f"Warning: '{target_col}' not found, using 'PHENOTYPE' column instead.")
             elif 'label' in dx_df.columns:
                 target_col = 'label'
                 logger.log(f"Warning: '{target_col}' not found, using 'label' column instead.")
             else:
                 raise ValueError(f"Target column ('{target_col}' or alternatives) not found in dx file.")

        y = dx_df[target_col].values
        unique_labels = np.unique(y)
        if not np.all(np.isin(unique_labels, [0, 1])):
             logger.log(f"Warning: Phenotype labels are {unique_labels}. Attempting conversion to 0/1. Check if this is correct.")
             if len(unique_labels) == 2:
                 y = (y == unique_labels.max()).astype(int)
                 logger.log(f"Converted labels {unique_labels} to {np.unique(y)}.")
             else:
                  logger.log("Warning: More than two unique labels found. Cannot automatically convert to binary 0/1.")

        logger.log(f"Loaded {len(y)} labels. Label distribution: {np.bincount(y)}")
        if len(y) != len(raw_df):
             logger.log(f"Error: Number of samples in raw data ({len(raw_df)}) does not match number of labels ({len(y)}).")
             logger.write()
             sys.exit(1)
    except FileNotFoundError:
        logger.log(f"Error: Diagnosis data file not found at {args.dx_data_file}")
        logger.write()
        sys.exit(1)
    except Exception as e:
        logger.log(f"Error loading or processing diagnosis data file: {e}")
        logger.write()
        sys.exit(1)

    df_local = local_analysis(
        raw_df=raw_df,
        y=y,
        window_size=args.window_size,
        prefix=args.prefix,
        tabnet_epoch=args.tabnet_epoch,
        tabnet_patience=args.tabnet_patience,
        n_splits=args.n_splits,
        logger=logger,
        result_dir=args.result_dir
    )

    logger.log(f"Loading annotations from: {args.annotation_file}")
    try:
        anno_raw_df = pd.read_csv(args.annotation_file)
        logger.log(f"Loaded {anno_raw_df.shape[0]} raw annotation entries.")
    except FileNotFoundError:
        logger.log(f"Error: Annotation file not found at {args.annotation_file}")
        logger.write()
        sys.exit(1)
    except Exception as e:
        logger.log(f"Error loading annotation file: {e}")
        logger.write()
        sys.exit(1)

    df_anno_transformed = transform_annotation_advanced(anno_raw_df, logger)
    ohe_file = os.path.join(args.result_dir, f"{args.prefix}_annotation_transformed.csv")
    df_anno_transformed.to_csv(ohe_file, index=False)
    logger.log(f"Transformed annotations saved -> {ohe_file}")

    df_global = calculate_global_annotation(
        raw_df=raw_df,
        y=y,
        anno_df=df_anno_transformed,
        prefix=args.prefix,
        tabnet_epoch=args.tabnet_epoch,
        tabnet_patience=args.tabnet_patience,
        n_splits=args.n_splits,
        logger=logger,
        result_dir=args.result_dir
    )

    logger.log("Combining local and global results...")
    if df_global.empty:
        logger.log("Warning: Global analysis returned no results. Proceeding without global scores.")
        snp_list = df_local['SNP'].unique() if not df_local.empty else []
        df_global = pd.DataFrame({'SNP': snp_list})

    gcols = [c for c in df_global.columns if c.endswith('_global_accuracy')]
    if gcols:
        df_global['global_accuracy_mean'] = df_global[gcols].mean(axis=1, skipna=True)
        logger.log(f"Calculated 'global_accuracy_mean' from {len(gcols)} columns.")
    else:
        logger.log("No global accuracy columns found to calculate mean. Setting 'global_accuracy_mean' to 0.")
        df_global['global_accuracy_mean'] = 0.0

    if not df_local.empty and 'SNP' in df_local.columns:
         df_global_merge = df_global[['SNP', 'global_accuracy_mean'] + gcols].copy()
         merged_df = pd.merge(
             df_local[['SNP', 'local_accuracy']],
             df_global_merge,
             on='SNP',
             how='outer'
         )
         merged_df['local_accuracy'] = merged_df['local_accuracy'].fillna(0.0)
         merged_df['global_accuracy_mean'] = merged_df['global_accuracy_mean'].fillna(0.0)
         for col in gcols:
             if col in merged_df.columns:
                 merged_df[col] = merged_df[col].fillna(0.0)
         logger.log(f"Merged local and global results into DataFrame with {len(merged_df)} SNPs.")
    elif not df_global.empty and 'SNP' in df_global.columns:
         logger.log("Local analysis did not produce results. Using only global results.")
         merged_df = df_global.copy()
         merged_df['local_accuracy'] = 0.0
    else:
         logger.log("Error: Neither local nor global analysis produced results with SNP information.")
         merged_df = pd.DataFrame()

    if merged_df.empty:
        logger.log("No results to combine or rank. Exiting analysis.")
        logger.write()
        sys.exit(1)

    df_alpha, best_alpha, best_score = alpha_search(
        merged_df=merged_df.copy(),
        raw_df=raw_df,
        y=y,
        logger=logger,
        top_n=args.alpha_top_n,
        n_splits=args.n_splits,
        step=args.alpha_step
    )
    alpha_csv = os.path.join(args.result_dir, f"{args.prefix}_alpha_search.csv")
    df_alpha.to_csv(alpha_csv, index=False)
    logger.log(f"Alpha search results saved -> {alpha_csv}")

    logger.log(f"Calculating final scores using best alpha = {best_alpha:.2f}")
    merged_df['final_score'] = best_alpha * merged_df['local_accuracy'] + (1 - best_alpha) * merged_df['global_accuracy_mean']
    out_combined = os.path.join(args.result_dir, f"{args.prefix}_combined_results.csv")
    merged_df_ranked = merged_df.sort_values('final_score', ascending=False).reset_index(drop=True)
    merged_df_ranked.to_csv(out_combined, index=False)
    logger.log(f"Combined results with final scores saved -> {out_combined}")

    pdf_path = os.path.join(args.result_dir, f"{args.prefix}_roc_curves.pdf")
    logger.log(f"Generating ROC curve PDF -> {pdf_path}")
    try:
        with PdfPages(pdf_path) as pdf:
            for n_val in args.roc_n_values:
                if n_val > 0:
                    plot_roc_for_n(
                        n=n_val,
                        ranked_df=merged_df_ranked,
                        raw_df=raw_df,
                        y=y,
                        pdf_object=pdf,
                        logger=logger,
                        n_splits=args.n_splits
                    )
                else:
                    logger.log(f"Skipping ROC plot for N={n_val} (must be positive).")
        logger.log("ROC curve PDF generation complete.")
    except Exception as e:
        logger.log(f"Error generating ROC curve PDF: {e}")

    logger.log("DuAL-Net Analysis Pipeline Finished.")
    logger.write()

if __name__ == "__main__":
    main()
